# Questioni etiche riguardo l'intelligenza artificiale

## Diritto d'autore e proprietà intellettuale
Le questioni in questo contesto sono due.

La prima riguarda ancora quello che l'intelligenza artificiale usa per soddisfare richieste generative. Utilizza una grande mole di dati, tra i quali anche quelli che sono protetti dal diritto d'autore. In questo modo possono analizzarli ed elaborarli senza che ci siano delle norme stringenti a riguardo quindi, in un certo senso, violando i diritti d'autore esistenti. 

In secondo luogo, c'è la questione riguardo la proprietà intellettuale delle opere generate con l'IA. 
Vi è mai capitato di far generare un’immagine a un’intelligenza artificiale?
Se la risposta è sì, allora nasce subito un’altra domanda : chi è l’autore di quell’immagine? Siete voi, che avete avuto l’idea e scritto il prompt, oppure è l’intelligenza artificiale, come ad esempio ChatGPT, che ha effettivamente prodotto l’immagine?

Come esempio ho provato a descrivere dettagliatamente una pianta dai poteri magici all'intelligenza artificiale, chiedendole se potesse generare un immagine.
La descrizione era la seguente:
*"La pianta presenta foglie rosso vivo, leggermente carnose, e steli sottili ma resistenti. I fiori, anch’essi rossi, emettono un leggero bagliore quando qualcuno si avvicina. La sua caratteristica più sorprendente è la capacità di percepire i pensieri vicini: reagisce con minimi movimenti delle foglie e leggere variazioni cromatiche, e può anche “parlare”, sussurrando ad alta voce i pensieri che riesce a captare, come se li trasformasse in racconti botanici. La sua presenza è discreta ma inquietante, attirando curiosità e rispetto da chi la osserva da vicino."*

Quello che ho ottenuto è la seguente immagine:
![alt text](Gemini_Generated_Image_xf87q0xf87q0xf87 copy.png) 

In questo contesto diventa essenziale rivedere il processo di valutazione di un'opera. Non basta più attribuire il valore a un prodotto solo sulla base del risultato, è necessario considerare e valutare anche il processo che ha portato alla sua creazione.

## Problema di BIBO (Bias In Bias Out)
Uno dei principali problemi etici dell’intelligenza artificiale riguarda il modo in cui viene addestrata. Un’IA, infatti, non nasce con una morale propria né con una comprensione innata del mondo: impara esclusivamente dai dati che le vengono forniti dagli esseri umani.

Questo significa che, se i dati di addestramento contengono pregiudizi, stereotipi o squilibri, l’IA tenderà a considerarli come se fossero “regole” corrette della realtà. Questo fenomeno è spesso riassunto con l’espressione Bias In, Bias Out: se inseriamo dati distorti, otterremo risultati distorti.

I pregiudizi nei dati possono nascere in diversi modi, ad esempio:
- Dati storici che riflettono discriminazioni del passato e che quindi non rappresentano una società equa;
- Rappresentazione non equilibrata dei gruppi: alcuni gruppi di persone sono poco presenti nei dataset, come è accaduto in certi sistemi di riconoscimento facciale addestrati soprattutto su volti caucasici;
- Pregiudizi umani: chi seleziona, etichetta o interpreta i dati può inconsapevolmente trasferire le proprie convinzioni personali all’IA.

Tutto ciò influisce direttamente sui risultati prodotti dai sistemi di intelligenza artificiale. Un esempio molto chiaro riguarda i software di riconoscimento facciale: il modo in cui vengono addestrati determina quanto bene (o male) riescono a riconoscere volti diversi.

Un caso famoso è quello di Joy Buolamwini, studentessa e ricercatrice del MIT. Durante un progetto si accorse quasi per caso che i software di riconoscimento facciale che stava utilizzando faticavano a rilevare il suo volto. In modo sorprendente, però, il sistema funzionava perfettamente quando lei indossava una maschera bianca.
Approfondendo la questione, Buolamwini scoprì che molti dataset usati per addestrare questi sistemi erano composti per oltre il 75% da uomini e per più dell’80% da persone con la pelle chiara. Di conseguenza, l’intelligenza artificiale era diventata molto “brava” a riconoscere uomini bianchi, ma molto meno efficace con donne e persone con la pelle scura.\
(Video di riferimento: https://www.youtube.com/watch?v=UG_X_7g63rY)

## L'ipersuasione (influenza mirata)
L'IA ha la capacità di selezionare le informazioni che un utente vede. Questo comportamento non sempre è un vantaggio. 
Quello che succede è che si va a creare una sorta di 'bolla di filtraggio' che potrebbe avere effetti negativi, come:
- rafforzamento di convinzioni e pregiudizi esistenti
- suggerire opzioni che modellano decisioni che possono essere banali ma anche decisive nella vita dei soggetti

Per capire ancora meglio quali sono i rischi di questo comportamento lo si può considerare nel contesto politico.


È noto il caso di Cambridge Analytica che mostra come l’uso dei dati personali possa influenzare la democrazia. Attraverso un semplice test della personalità su Facebook, una società privata è riuscita a raccogliere, sfruttando una falla del sistema, i dati di milioni di persone, molte delle quali non avevano mai dato il consenso.

Questi dati sono stati utilizzati per creare profili psicologici dettagliati degli utenti tramite il cosiddetto microtargeting psicografico, basato sul modello OCEAN. In questo modo è stato possibile inviare messaggi politici altamente personalizzati.

Le informazioni raccolte sono state impiegate per influenzare eventi politici importanti, come le elezioni presidenziali statunitensi del 2016 e il referendum sulla Brexit, mostrando come i dati digitali possano essere usati non solo a fini commerciali, ma anche per condizionare le scelte politiche e civiche delle persone.\
(Video di riferimento: https://www.youtube.com/watch?v=KZKzI4_f2xI)


## Privacy
Un altro aspetto fondamentale dell’etica dell’intelligenza artificiale riguarda la privacy, cioè la tutela dei dati personali.

L’IA è in grado di raccogliere enormi quantità di dati, incrociarli tra loro, analizzarli e individuare connessioni che spesso sfuggono all’essere umano. Questa capacità può essere molto utile, ad esempio quando si fa una ricerca per un compito di scienze. Tuttavia, è importante chiedersi se siamo sempre sicuri che questo uso dei dati sia desiderabile e sicuro?

Il problema nasce dal fatto che i sistemi di intelligenza artificiale accumulano, analizzano e manipolano grandi quantità di informazioni, spesso per conto di un numero limitato di grandi aziende. Questo solleva interrogativi importanti su chi controlla questi dati, come vengono utilizzati e se le persone sono davvero consapevoli di ciò che stanno condividendo.\
Il rischio è che dati personali sensibili, vengano usati senza un controllo adeguato o senza il consenso informato degli utenti.

Un esempio particolarmente delicato riguarda i sistemi di IA utilizzati in ambito sanitario. In questo caso, i dati analizzati possono includere informazioni mediche estremamente private. La priorità assoluta di questi sistemi deve quindi essere la tutela della riservatezza del paziente, garantendo che i dati siano protetti, utilizzati solo per scopi legittimi e accessibili esclusivamente a persone autorizzate.

Nello specifico il caso di Project Nightingaleun progetto avviato da Google (attraverso la sua divisione cloud) in collaborazione con Ascension, uno dei più grandi sistemi sanitari negli USA, con oltre 2.6 milioni di pazienti coinvolti.

L’obiettivo dichiarato era sviluppare strumenti di analisi dei dati sanitari e migliorare l’efficienza clinica, ad esempio:
- Migliorare la gestione delle cartelle cliniche elettroniche (EHR, Electronic Health Records).
- Fornire analisi predittive per i medici (rischio di malattie, ottimizzazione dei trattamenti).
- Supportare decisioni cliniche basate sui dati.

Il progetto è stato criticato per diversi motivi legati alla privacy e alla trasparenza:
- Google aveva accesso a informazioni estremamente dettagliate dei pazienti: nomi, date di nascita, referti medici, dati di laboratorio, storie cliniche complete.
In totale, milioni di cartelle cliniche erano disponibili a dipendenti di Google senza il consenso diretto dei pazienti.
- Né i pazienti né i medici coinvolti erano stati formalmente informati o avevano dato il loro consenso scritto.
Questo ha sollevato questioni etiche: anche se l’accesso era legale secondo la normativa HIPAA (Health Insurance Portability and Accountability Act), la percezione pubblica era di una violazione della privacy.
- Dipendenti non clinici di Google potevano teoricamente accedere ai dati, il che ha alimentato timori su possibili usi commerciali, profilazioni o persino pubblicità mirata (anche se l’azienda ha negato qualsiasi utilizzo dei dati per scopi pubblicitari).

(https://tg24.sky.it/tecnologia/2019/11/12/google-dati-sanitari-pazienti)

## L'impatto ambientale
Per funzionare, le intelligenze artificiali hanno bisogno di un enorme quantità di energia, per diverse cose tra cui il raffreddamento, la parte operativa e tutte le funzioni necessarie al loro funzionamento.

Il problema non è solo quanta energia si usa, ma dove la si prende. Se il data center è in un paese che produce energia dal carbone, ogni tua domanda all'IA emette CO2. Se invece è alimentato da pannelli solari o turbine eoliche, l'impatto è molto minore.

Inoltre, c'è il problema dell'acqua: per raffreddare i server, i data center consumano milioni di litri d'acqua dolce, che spesso evapora e non viene recuperata immediatamente.

## Deepfake
Con deep fake si intende la pratica di generazione di contenuti multimediali (video, immagini o audio) realistici ma artificiali, ottenuti mediante tecniche di intelligenza artificiale, in particolare algoritmi di deep learning, che consentono di sostituire, alterare o simulare l’identità, la voce o i movimenti di una persona reale, producendo rappresentazioni difficilmente distinguibili da contenuti autentici. 

Tali contenuti possono essere impiegati in contesti leciti (ad esempio intrattenimento o sperimentazione artistica), ma diventano problematici quando vengono utilizzati senza consenso o con finalità ingannevoli, poiché compromettono l’affidabilità dell’informazione, violano i diritti individuali e possono generare gravi conseguenze sociali, giuridiche ed etiche. 

Un esempio è il caso del 2018 di un video deepfake di Barack Obama che è stato pubblicato su YouTube da Jordan Peele insieme a BuzzFeed, in cui l'ex presidente sembra pronunciare frasi offensive e ingannevoli. L’intento dichiarato era proprio quello di mostrare i pericoli della manipolazione video e dell’IA generativa.
(https://en.wikipedia.org/wiki/Deepfake)

Il caso peggiore è quando viene utilizzato con Deepfake pornografici. 
Il caso della ragazza di Foggia, vittima di questa pratica.
(https://www.immediato.net/2025/10/24/caso-deepfake-a-foggia-ci-sono-piu-indagati-arianna-a-storie-italiane-racconta-il-suo-incubo)



## Riferimento bibliografico
*Floridi, L. (2025). La differenza fondamentale. Artificial Agency: una nuova filosofia dell'intelligenza artificiale*. Mondadori.
